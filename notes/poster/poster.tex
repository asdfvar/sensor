\documentclass[unknownkeysallowed,final]{beamer}
\usetheme{RJH} %RJH}
\usepackage[orientation=landscape,size=a2,scale=1.4,debug]{beamerposter}
\usepackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

\usepackage{amsmath,amsthm, amssymb, latexsym}
\usepackage{multirow}
\usepackage{rotating}

\usepackage{array,booktabs,tabularx}
\newcolumntype{Z}{>{\centering\arraybackslash}X} % centered tabularx columns
\newcommand{\pphantom}{\textcolor{ta3aluminium}} % phantom introduces a vertical space in p formatted table columns??!!
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}

% Poster size can be: 36" wide by 44" tall

\title{\normalsize{Real-Time Activity Classification by Matched Filtering using Body-Worn Accelerometers}}
\author{\small{Craig~Euler craig.euler@gmail.com, C.T.~Lin ct.lin@csun.edu,\\Bryan~Juarez bryan.juarez.974@my.csun.edu, Melissa~Flores melissa.flores.455@my.csun.edu\\College of Engineering and Computer Science\\California State University, Northridge}}

\footer{}
\date{}

\begin{document}
\tiny{}
\begin{frame}{} 

%%
%% Text block
%%
\begin{textblock}{19.5}(1, 7)

\begin{block}{\small{Abstract}}
Having information of a user's activity can provide great use in modern-day devices such as ones that track and monitor the user's activity and fitness.
In this paper, we demonstrate activity classification performance of the matched-filtering method with data obtained from a three-axis accelerometer worn by the user.
We also show the real-time processing capability of our algorithms on the MSP432P401R low-powered microcontroller.
Dimensionality reduction with principal component analysis (PCA) \cite{bishop_2006} is a data compression technique we use to improve our processing throughput which, inherently, has the added benefit of making     our data invariant to sensor orientation.
Data decimation in time is an additional throughput enhancement that we apply early to our data.
We make use of an instance-based learning algorithm to train the device to learn the individual's motion patterns and store that information as activity templates for use in our matched-filter.
\end{block}

\begin{block}{\small{Introduction}}
Various methods for activity classification using body-worn sensors exist.
Custom Decision Tree, Automatically Generated Decision Tree, and Artificial Neural Networks \cite{parkka_ermes_korpipaa_mantyjarvi_peltola_korhonen_2006} have been explored as well as other frequency based classification methods \cite{sharma_purwar_lee_lee_chung_2008}.
The use of the matched-filter method for classification has been studied in the past \cite{giannakis_tsatsanis_1990} and we seek to adopt this method for our use to identify an individual's activity for the purpose of real-time processing on a low-powered micro-controller.
The methods used for our algorithms are designed for computational efficiency to accommodate the resource limitations of a microcontroller. In this paper, we use the MSP432 microcontroller for our timing results.

The algorithms described in this paper are used in two main modules: The activity classification module for real-time processing and the training module to train the device to identify the user's activity.

The training module does not need to be performed in real time.
This algorithm is the most demanding on computational resources and is only executed when training or re-training the unit to identify the user's activity, which means that this can be performed on an external device such as a smart phone or tablet.
The purpose of the training module is to create the reference signals to be used on the microcontroller for the real-time activity classification processing.

The real-time activity classification module is intended to be performed on a low-powered microcontroller.
This module reads in the references stored on the unit to be used as templates that the matched-filter will use for determining the individual's activity.
Both of these modules use the same principles and hence the same algorithm kernels.
\end{block}

%%
%% Activity Classification Method
%%
\begin{block}{\small{Activity Classification Method}}

The matched-filter method is commonly used for extracting a signal out of noisy data.
We use this method for identifying a user's activity from data generated by body-worn accelerometers.
Because the matched-filter method requires use of an existing reference template to identify the desired signal, we must develop our template from a training set.
We will discuss how we develop our set of templates for use in our matched-filter in section \ref{sec:training}.

The matched-filter is based on the cross correlation of the data $\textbf{y} = \{y\}_{k=0}^{N-1}$ with the reference signal $\textbf{x} = \{x\}_{k=0}^{M-1}$.
The cross correlation of $\textbf{x}$ with $\textbf{y}$ is defined as:
%
\begin{equation} \label{eq:cross_correlation}
(\textbf{x} \star \textbf{y}) := \left \{\sum_{i=0}^{M-1}x_{i} y_{i+k} \right \}_{k=0}^{N-M-1}
\end{equation}
%
which is simply a moving dot product.
We can then normalize \eqref{eq:cross_correlation} by the following:
%
\begin{equation} \label{eq:norm_cross_correlation}
\widehat{(\textbf{x} \star \textbf{y})}_k := \frac{(\textbf{x} \star \textbf{y})_k}{||\textbf{x}|| \ || \widetilde{\textbf{y}}_k || }, \quad k = 0,1,...,N-M-1
\end{equation}
%
where $|| \cdot ||$ is the usual $\ell_2$ norm and $\widetilde{\textbf{y}}_k = \{y_p\}_{p=k}^{M+k-1}$.
\eqref{eq:norm_cross_correlation} is now bounded by $[-1,1]$ and has any bias toward high energy removed.
We improve our computational efficiency by applying the convolution theorem to \eqref{eq:cross_correlation} and replacing it with:
%
\begin{equation} \label{eq:conv_theorem}
(\textbf{x} \star \textbf{y}) := FFT^{-1}(FFT(\textbf{x}') FFT(\textbf{y})^*)
\end{equation}
%
where $\textbf{x}'$ is $\textbf{x}$ that has been zero padded to the length of $\textbf{y}$ with the added restriction that the length of $\textbf{x}$ is no more than the length of $\textbf{y}$ and $FFT$ is the Fast Fourier Transform algorithm.
We are most interested where our reference $\textbf{x}$ best matches our data $\textbf{y}$.
We use the resulting max correlation to define our metric for  our matched-filter output as:
%
\begin{equation} \label{eq:matched_filter}
MF_{\textbf{x}}(\textbf{y}) := \underset{k \in [0, M-N)}{max} \left \{\widehat{(\textbf{x} \star \textbf{y})}_k \right \}
\end{equation}
%
If \eqref{eq:matched_filter} passes a set threshold, then the corresponding signal $\textbf{y}$ is classified as a match to the reference $\textbf{x}$.

Applying the convolution theorem to $\eqref{eq:conv_theorem}$ changes the computational complexity or the cross correlation from $\mathcal{O}(n^2)$ to $\mathcal{O}(n log_2n)$.
Despite this reduction in processing cost, the FFTs are still an expensive algorithm for real-time processing on limited hardware resources.
Sections \ref{sec:dim_red} and \ref{sec:decimation} will go into methods that will help reduce the computational burden of the matched-filter.
\end{block}
\end{textblock}

\begin{textblock}{18}(21.8,4)

\begin{block}{\small{Dimensionality Reduction}}
Classification methods that make use of multi-dimensional feature spaces have increased in popularity and many methods exist \cite{survey_multi_view_machine_learning}.
We manage to reduce our computational resources and remove our dependence on sensor orientation at the same time through a process of dimensionality reduction with principal component analysis (PCA) \cite{bishop_2006}.
Similar to other methods that apply subspace projections \cite{low_rank_subspace}, we seek to analyze our data in a lower dimensional subspace by a projection onto the two-dimension plane that best represents our data.
We assume that over a short period of time, a user's limb movement is constrained mostly to a two-dimensional plane.
By identifying this plane of motion, we then project the acceleration data onto this plane and re-define our new axis accordingly.
%
If we let $\textbf{a} = \{a_i\}_{i=0}^{N}$ represent our demeaned acceleration data (where each $a_i$ is represented as a column) over a specified time window in three dimensions, then the covariance matrix of our acceleration data is represented by the real-symmetric $3 \times 3$ matrix $Q = \textbf{a} \textbf{a}^T$.
The resulting unit length column eigenvectors of $Q$ are $V = (\textbf{v}^1,\textbf{v}^2,\textbf{v}^3)$ with their respective eigenvalues $\lambda^1 \geq \lambda^2 \geq \lambda^3$ of this matrix are orthonormal and point in the directions where the data varies most in descending order for each orthogonal direction.
Here, we use superscripts to identify the axis directions in ascending order to represent the primary, secondary, and tertiary principal axis directions respectively.
%
\begin{equation} \label{eq:project}
   \textbf{a}' := V^T \textbf{a}
\end{equation}
%
We now represent our acceleration data as $\textbf{a}'$ after projecting our acceleration data $\textbf{a}$ onto our rotated axis $V$ by \eqref{eq:project}. If the data is mostly dominate in the two primary dimensions, i.e. $\lambda^3 << \lambda^2$, then it is safe to assume that
%
\begin{equation} \label{eq:magnitude}
||(a_x,a_y,a_z)||^2 = ||(a'^1,a'^2,a'^3)||^2 \sim = (a'^1)^2 + (a'^2)^2
\end{equation}
$ \forall a \in \textbf{a}. $ \\
%
Where equality comes from the orthonormality of $V$.
By now, we have preserved as much information that is contained in our acceleration data as possible in the process of reducing our dimensions from three to two.

In this paper, we compute the matched-filter output for two dimensions by generalizing \eqref{eq:matched_filter} by augmenting our reference and signal with both dimensions appropriately for normalization and adding together the cross correlations as:
%
\begin{equation} \label{eq:cross_correlation_2}
\widehat{(\textbf{x}^{1,2} \star \textbf{y}^{1,2})}_k := \frac{(\textbf{x}^1 \star \textbf{y}^1)_k + (\textbf{x}^2 \star \textbf{y}^2)_k}{||[ \textbf{x}^1, \textbf{x}^2 ]|| \ || [ \widetilde{\textbf{y}}_k^1, \widetilde{\textbf{y}}_k^2 ] || },
\end{equation}
%
for $ k = 0,1,...,N-M-1 $. \\
%
\begin{equation} \label{eq:matched_filter_2}
MF_{\textbf{x}}^2(\textbf{y}) := \underset{k \in [0, M-N)}{max} \left \{\widehat{(\textbf{x}^{1,2} \star \textbf{y}^{1,2})}_k \right \}
\end{equation}
%
This works because both dimensions are always in phase with each other for any given activity and therefore the match will happen in sync. Here, the superscripts $1$ and $2$ represent the primary and secondary principal components of our data respectively.
%
\end{block}

\begin{block}{\small{Decimation}}
Decimation is the first step applied to our data before any additional processing takes place.
Decimation is a process of reducing the sampling rate of the signal by filtering to mitigate any possible distortion effects due to aliasing followed by downsampling.
Because real-time performance is important, we decimate our signal for the purpose of improving our throughput.

For a decimation value of $D$, we define our triangle filter with the following weights:
%
\begin{equation} \label{eq:triangle_filter_weights}
w_k^D :=
\begin{cases}
  \frac{D+k}{D^2}, & \text{if }\ -D < k \leq 0 \\
  \frac{D-k}{D^2}, & \text{if } 0 < k < D \\
  0, & \text{otherwise}
\end{cases}
\end{equation}
%
and convolve it with our data to get
%
\begin{equation} \label{eq:convolve}
s'_k := \sum_{i=0}^{N} s_i w_{k-i}
\end{equation}
%
followed by a downsample
%
\begin{equation} \label{eq:downsample}
s''_k := s'_{(k+1)D-1}, \quad k = 0, 1, 2, ..., N/D.
\end{equation}
%
The processing cost is reduced by only processing points in \eqref{eq:convolve} that we keep in \eqref{eq:downsample}.
\end{block}


\end{textblock}

%%
%% Text block
%%
\begin{textblock}{17.3}(41,4)

\begin{block}{\small{Training the Module}}

We are able to identify an individual's activity by matching the signal to a reference template.
For our application, we develop a reference template for our matched-filter directly from the person's motion signature.
We do this by having the individual perform the specific activity over a specified amount of time and select the sub-interval of time that we find best represents the user's motion signature.
For a given activity, we attempt to find the representative periodic signal $\textbf{s}$ of our individual's activity to be our template for our matched-filter.

If $\textbf{t} = \{t_k\}_{k=0}^{N_t}$ is our training set over $N_t$ data points, we define a collection of subsets $X$ of $\textbf{t}$:
%
\begin{equation} \label{eq:X_subsets_of_training_eq}
X := \left \{ \{t_k\}_{k=p}^{p+M-1} | p=mI \right \}
\end{equation}
%
and $Y$ of $\textbf{t}$:
%
\begin{equation} \label{eq:Y_subsets_of_training_eq}
Y := \left \{ \{t_k\}_{k=p}^{p+N-1} | p=nI \right \}
\end{equation}
%
where $M \leq N$, $I$ is a positive, fixed integer, and $(m,n) \in \mathbb{Z}^2$ where $(m,n)$ vary such that $0 \leq (m,n)I$ and $(m,n)I + (M,N) - 1 \leq N_t$. Then we can determine the representative signal $\textbf{s} \in X$ for our activity that satisfies the condition:
%
\begin{equation} \label{eq:s_condition}
\sum_{y \in Y}MF^2_{\textbf{s}}(\textbf{y}) \geq \sum_{y \in Y}MF^2_{\textbf{x}}(\textbf{y}), \quad \forall \textbf{x} \in X
\end{equation}

We chose this method for its simplicity and robustness against outliers in our training set. We anticipate that the user will have idle motion at the start and end of their training duration when they activate the training mode and with possible disruptions during the process.

%\begin{center}
% \includegraphics[scale=.75]{reference_pulse_32Hz_4sec_8downsample_2.png}
%\end{center}
\end{block}

\begin{block}{\small{Performance}}
%The Kinetisense sensor was used for the algorithm performance analysis in this paper.
%This unit has a three-axis accelerometer with an acceleration range of -5g to +5g and a sampling rate of 128Hz.
%Samples of walking, jogging, and bicycling have been recorded with this unit attached to the ankle of the user.
%A separate instance of walking data was used specifically for training the classifier.
%%
%\begin{figure}[!ht]
%  %\captionsetup[subfigure]{labelformat=empty}
%  \centering
%  \subfloat[\label{fig:frac_walking_jogging}]{\includegraphics[width=0.45\textwidth]{fraction_threshold_walking_to_jogging.png}}
%  \quad
%  \centering
%  \subfloat[\label{fig:frac_walking_bicycling}]{\includegraphics[width=0.45\textwidth]{fraction_threshold_walking_to_bicycling.png}}
%  \centering
%  \quad
%  \subfloat[\label{fig:roc_walking_jogging}]{\includegraphics[width=0.45\textwidth]{ROC_walk_to_run.png}}
%  \centering
%  \quad
%  \subfloat[\label{fig:roc_walking_bicycling}]{\includegraphics[width=0.45\textwidth]{ROC_walk_to_bike.png}}
%  \caption{Matched-filter performance comparing jogging and bicycling data against a reference for walking. Results produced at decimation 8. Figures (c) and (d) are the corresponding ROC curves produced from (a) and (b) respectively.}
%  \label{fig:MF_performance}
%\end{figure}
%
%Figures \ref{fig:frac_walking_jogging} and \ref{fig:frac_walking_bicycling} show the fraction of the correlation results that exceed the given correlation threshold when comparing the walking data reference to the given activity.
%As we would expect, the walking data reference matches better to walking data than to the other activities. The tail in figure \ref{fig:frac_walking_jogging} is attributed to brief moments of walking that was recorded during the jogging data collection.
%ROC curves were produced from these corresponding results in figures \ref{fig:roc_walking_jogging} and \ref{fig:roc_walking_bicycling} to illustrate the effectiveness of the matched-filter as a classifier.
%Because the bicycling motions are considerably different from walking relative to jogging, we would expect better performance from our classifier for bicycling.
%
%To assess the real-time processing potential of the algorithms outlined in this paper, a throughput assessment was performed on the MSP432P401R microcontroller with varying parameters.
%The Texas Instruments - MSP432P401R is a low power plus performance microcontroller that is based on the ARM 32-Bit Cortex-M4F processor.
%Some of the notable features of the MSP432P410R is having a clock frequency of 48 MHz, 64 kB SRAM, 256 kB Flash Memory, an IEEE 754-compliant single-precision Floating-Point Unit (FPU), and Memory Protection Unit (MPU).
%The single-precision processing capability on this unit provides us with the dynamic range necessary to process the algorithms that fixed-point processing cannot provide.
%The 64 kB of SRAM provides sufficient space to process the results found in figure \ref{fig:timing} and
%the 256 kB is more than enough flash memory to store the program and data references.
%The clock frequency operates at 48 MHz which drives the execution speed.
%
%\begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.45\textwidth]{reference_pulse_32Hz_4sec_8downsample_2.png}
%   \caption{Oscilloscope output used for measuring the processing time of the algorithms on the MSP432. This is a case setup of 32Hz sampling frequency over a 4 second time window with 2 references at decimation 8 showing the voltage output over time.
%Execution time is measured from start to end of the ``high'' (3.3 Volts) time interval.}
%   \label{fig:time_measure}
%\end{figure}
%%
%The ``Analog Discovery 100MSPS USB Oscilloscope and Logic Analyzer'' oscilloscope was used for measuring the timing of the algorithms on the microcontroller.
%When testing the execution speed, one of the General-Purpose Input/Output (GPIO) pins was connected to a Light-emitted diode (LED) that activates at 3.3 Volts (set high).
%This is activated immediately before the beginning of decimation (start of algorithm processing) and deactivated immediately at the end of the matched-filtering (end of algorithm processing).
%The execution time is then recorded on the oscilloscope to determine how long the GPIO was set high.
%An example can be seen in Figure \ref{fig:time_measure}.
%
%\begin{figure}[!ht]
%   \centering
%   \subfloat[\label{fig:multiple_comparison}]{\includegraphics[width=0.45\textwidth]{more_time_plots.png}}
%   \quad
%   \subfloat[\label{fig:timing_refs}]{\includegraphics[width=0.45\textwidth]{one_and_two_references.png}}
%   \caption{Timing results with varying sampling frequencies, time-window lengths, and number of references used for the matched-filter on the MSP432 microcontroller processor. The time window used for (b) is 4.0 seconds with timings measured at sampling frequencies of $16, 32, 62, 64, 75, 100,$ and $128$ Hz.}
%   \label{fig:timing}
%\end{figure}
%%
%The throughput performance was assessed on the MSP432 with varying parameters and randomized data.
%This can be accomplished because the processing time is not affected by the data and this also simplifies this aspect of testing.
%Figure \ref{fig:multiple_comparison} shows the execution time with varying time-window lengths and sampling frequencies while maintaining two references used in the matched-filter process while operating at decimation $2$.
%Because the FFT dominates the processing load, and a custom FFT was written specifically for this unit and optimized for even-powered factors, one expects best throughput at radix 2 valued time-window lengths and sampling frequencies followed by other even-valued, non-radix 2, time-lengths and sampling frequencies. Both $62$Hz and $50$Hz have only one factor of $2$ hence the expectation that a sampling frequency of $62$Hz processing time is longer than that of the $50$Hz.
%Figure \ref{fig:timing_refs} shows the execution time over a $4.0$ second time window at decimation $8$ while varying the sampling frequency and number of references.
%The processing that happens on the data before matched-filtering doesn't change the execution time with the number of references.
%Therefore, one can observe that matched-filtering accounts for roughly $80$\% of the processing time.
%%
%\begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.99\textwidth]{flops_graph.png}
%   \caption{An estimate of the processing distribution based on flop counts. This estimate was obtained from analyzing the source code that was used for the performance assessment. The cost of the FFT is based on the $2.5 N log_2 N$ mflops estimate for real-FFTs for FFTW \cite{FFTW} execution which was used for this performance assessment. A sampling rate of $128$ Hz with the matched-filter applied over a $4$ second time window and $1.5$ seconds of reference data was used.}
%   \label{fig:flops_count}
%\end{figure}
%
%An assessment of the processing costs associated with each algorithm in the process was made as seen in figure \ref{fig:flops_count}.
%In addition, timing tests were done for comparison on an Intel i7-4500U 1.80 GHz processor.
%Processing time came out to $12.0$ micro-seconds for dimensionality reduction processing and $114.0$ micro-seconds for matched-filter processing on the remaining two dimensions.
%This proportion shows relative consistency with figure \ref{fig:flops_count} as we would expect.
%FFT calls account for the bulk of the processing distribution and are part of the main processing kernel for the matched-filtering process.
\end{block}
%
\begin{block}{\small{Conclusion}}
%We have demonstrated excellent activity classification capability of the matched-filter method using data obtained from a Kinetisense tri-axis accelerometer sensor.
%We have shown that real-time processing on a low-powered micro-controller can be achieved with timing results obtained from the MSP432 microcontroller.
%Timing results have demonstrated that radix 2 time-window lengths and frequency values result in faster processing times as expected given that the FFTs account for a significant amount of the processing time.
%We increased our throughput with the use of data-compression techniques, namely decimation and dimensionality reduction, which reduced our computational requirements.
%The references used in the matched-filter are obtained from the training module where a training set is provided by the user through some specified activity and the best representative motion signature of the individual is extracted through the use of an instance-based learning algorithm.
\end{block}
%
\begin{block}{\small{References}}
\bibliographystyle{plain}
\bibliography{reference}
\end{block}
\end{textblock}

\end{frame}
\end{document}
